{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Questions 16-20, you will play with the decision stump algorithm.\n",
    "\n",
    "In class, we taught about the learning model of \"positive and negative rays'' (which is simply one-dimensional perceptron) for one-dimensional data. The model contains hypotheses of the form:\n",
    "\n",
    "$h_{s,\theta}(x)=s⋅sign(x−\theta).$\n",
    "\n",
    "The model is frequently named the \"decision stump'' model and is one of the simplest learning models. As shown in class, for one-dimensional data, the VC dimension of the decision stump model is 2.\n",
    "\n",
    "In fact, the decision stump model is one of the few models that we could easily minimize Ein efficiently by enumerating all possible thresholds. \n",
    "\n",
    "In particular, for N examples, there are at most 2N dichotomies (see page 22 of lecture 5 slides), and thus at most 2N different Ein values. We can then easily choose the dichotomy that leads to the lowest Ein, where ties an be broken by randomly choosing among the lowest Ein ones. The chosen dichotomy stands for a combination of some \"spot\" (range of \theta) and s, and commonly the median of the range is chosen as the \theta that realizes the dichotomy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decision Stump\n",
    "class DS:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.error_rate = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        assert len(X) == len(y)\n",
    "        X_sorted = sorted(X)\n",
    "        for s in [-1, 1]:\n",
    "            for i in range(len(y)):\n",
    "                if i == len(y) - 1:\n",
    "                    theta = X_sorted[i] + 1\n",
    "                else:\n",
    "                    theta = X_sorted[i] + X_sorted[i + 1] / 2\n",
    "                predictions = np.apply_along_axis(lambda x: s * np.sign(x - theta), 0, X)\n",
    "                error_rate = 1 - sum(predictions == y) / len(y)\n",
    "                if self.error_rate is None or self.error_rate > error_rate:\n",
    "                    self.s = s\n",
    "                    self.theta = theta\n",
    "                    self.error_rate = error_rate\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return np.apply_along_axis(lambda x: self.s * np.sign(x - self.theta), 0, X)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        assert len(X) == len(y)\n",
    "        predictions = np.apply_along_axis(lambda x: self.s * np.sign(x - self.theta), 0, X)\n",
    "        return sum(predictions == y) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you are asked to implement such and algorithm and run your program on an artificial data set. First of all, start by generating a one-dimensional data by the procedure below:\n",
    "\n",
    "(a) Generate x by a uniform distribution in [−1,1].\n",
    "\n",
    "(b) Generate y by $f(x)=\\widetilde s(x)$ + noise where $\\widetilde s(x)=sign(x)$ and the noise flips the result with 20% probability.\n",
    "\n",
    "For any decision stump $h_{s,\theta}$ with \theta∈[−1,1], express $E_{out}(h_s,\theta)$ as a function of \theta and s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Consider the bin model for a hypothesis h that makes an error with probability \mu in approximating a deterministic target function f (both h and f outputs {−1,+1}). \n",
    "\n",
    "If we use the same h to approximate a noisy version of f given by\n",
    "\n",
    "$P(x,y) = P(x)P(y|x)$\n",
    "\n",
    "$P(y|x) = \n",
    "\\begin{cases}\n",
    "    \\lambda, \\text{ if f(x)=y} \\\\\n",
    "    1 - \\lambda, \\text{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "The probability of error that h makes in approximating the noisy target y is given by:\n",
    "1. Correct prediction and not flipped by noise\n",
    "2. Incorrect prediction but flipped by noise\n",
    "\n",
    "Henceforth $\lambda\mu+(1−\lambda)(1−\mu)$\n",
    "\n",
    "In this case, the noisy parameter \lambda is 0.8, whereas the error parameter is given by:\n",
    "\n",
    "$\\mu = \n",
    "\\begin{cases}\n",
    "    |\\theta| / 2, \\text{ if s > 0} \\\\\n",
    "    (2 - |\\theta|) / 2, \\text{ otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "Thus the one-liner $\\mu = (1 - s + s|\\theta|) / 2$\n",
    "\n",
    "Substitute \lambda and \mu in the formula, we get the overall error rate:\n",
    "\n",
    "$0.5+0.3s(|\theta|-1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    \n",
    "    def __init__(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def generate(self, N, seed=None):\n",
    "        if not seed is None:\n",
    "            random.seed(seed)\n",
    "            \n",
    "        X = np.array([])\n",
    "        for _ in range(N):\n",
    "            X = np.append(X, random.uniform(-1, 1))\n",
    "        \n",
    "        y = np.array([])\n",
    "        for idx in range(N):\n",
    "            t = np.sign(X[idx])\n",
    "            if random.uniform(0, 1) < self.epsilon:\n",
    "                t = -t\n",
    "            y = np.append(y, t)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a data set of size 20 by the procedure above and run the one-dimensional decision stump algorithm on the data set. Record Ein and compute Eout with the formula above. Repeat the experiment (including data generation, running the decision stump algorithm, and computing Ein and Eout) 5,000 times. What is the average Ein? Please choose the closest option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average Ein is 0.18269000000000016\n"
     ]
    }
   ],
   "source": [
    "gen = Generator(0.2)\n",
    "avg_Ein = 0\n",
    "for i in range(5000):\n",
    "    X, y = gen.generate(20, seed=i)\n",
    "    ds = DS().fit(X, y)\n",
    "    avg_Ein += ds.error_rate\n",
    "avg_Ein /= 5000\n",
    "print('average Ein is {}'.format(avg_Ein))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing from the previous question, what is the average Eout? Please choose the closest option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average Eout is 0.2792910642952312\n"
     ]
    }
   ],
   "source": [
    "avg_Eout = 0.5 + 0.3 * ds.s * (abs(ds.theta) - 1);  \n",
    "print('average Eout is {}'.format(avg_Eout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision stumps can also work for multi-dimensional data. In particular, each decision stump now deals with a specific dimension i, as shown below.\n",
    "\n",
    "$h_{s,i,\theta}(x)=s⋅sign(x_i−\theta)$\n",
    "\n",
    "Implement the following decision stump algorithm for multi-dimensional data:\n",
    "\n",
    "a) for each dimension i=1,2,⋯,d, find the best decision stump $h_{s,i,\theta}$ using the one-dimensional decision stump algorithm that you have just implemented.\n",
    "\n",
    "b) return the \"best of best\"' decision stump in terms of Ein. If there is a tie , please randomly choose among the lowest-Ein ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multi-Dimensional Decision Stump\n",
    "class MDDS:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.dimension = None \n",
    "        self.error_rate = None\n",
    "        self.ds = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        assert X.shape[0] == y.shape[0]\n",
    "        for dimension in range(X.shape[1]):\n",
    "            ds = DS().fit(X[:, dimension], y)\n",
    "            if self.error_rate is None or ds.error_rate < self.error_rate:\n",
    "                self.ds = ds\n",
    "                self.error_rate = ds.error_rate\n",
    "                self.dimension = dimension\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.ds.predict(X[:, self.dimension])\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        assert len(X) == len(y)\n",
    "        predictions = self.ds.predict(X[:, self.dimension])\n",
    "        return sum(predictions == y) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data $D_{train}$ is available at:\n",
    "\n",
    "https://www.csie.ntu.edu.tw/~htlin/mooc/datasets/mlfound_math/hw2_train.dat\n",
    "\n",
    "The testing data $D_{test}$ is available at:\n",
    "\n",
    "https://www.csie.ntu.edu.tw/~htlin/mooc/datasets/mlfound_math/hw2_test.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('hw2_train.dat')\n",
    "X_train = data[:, :-1]\n",
    "y_train = data[:, -1]\n",
    "\n",
    "data = np.genfromtxt('hw2_test.dat')\n",
    "X_test = data[:, :-1]\n",
    "y_test = data[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithm on the $D_{train}$. Report the Ein of the optimal decision stump returned by your program. Choose the closest option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal Ein is 0.26\n"
     ]
    }
   ],
   "source": [
    "mdds = MDDS().fit(X_train, y_train)\n",
    "print('optimal Ein is {}'.format(mdds.error_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated Eout is 0.366\n"
     ]
    }
   ],
   "source": [
    "print('estimated Eout is {}'.format(1 - mdds.accuracy(X_test, y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
