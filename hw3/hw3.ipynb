{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Questions 13-15, consider the target function:\n",
    "\n",
    "$f(x_1,x_2)=sign(x^2_1+x^2_2−0.6)$\n",
    "\n",
    "Generate a training set of $N = 1000$ points on $\\chi =[−1,1] \\times [−1,1]$ with uniform probability of picking each $\\mathbf{x} \\in \\chi$\n",
    "\n",
    "Generate simulated noise by flipping the sign of the output in a random $10\\%$ subset of the generated training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$13.$ Carry out Linear Regression without transformation, \n",
    "\n",
    "i.e. with feature vector $(1, x_1, x_2)$ to find the weight $\\mathbf{w}_{\\rm lin}$ and use $\\mathbf{w}_{\\rm lin}$ directly for classification. \n",
    "\n",
    "What is the closest value to the classification (0/1) in-sample error $E_{\\rm in}$? \n",
    "\n",
    "Run the experiment 1000 times and take the average $E_{\\rm in}E$ in order to reduce variation in your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "def f(x):\n",
    "    return x[0]**2 + x[1]**2 - 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average in-sample error: 0.504855\n"
     ]
    }
   ],
   "source": [
    "e = []\n",
    "# run for 1000 times\n",
    "for _ in range(1000):\n",
    "    # Generate the training set\n",
    "    X_train = np.random.uniform(-1, 1, size=(1000, 2))\n",
    "    y_train = np.sign(list(map(f, X_train)))\n",
    "    X_train = np.insert(X_train, 0, 1, axis=1)\n",
    "\n",
    "    # Flip 10% of the signs\n",
    "    flip_ids = np.random.choice(1000, 100, replace=False)\n",
    "    y_train[flip_ids] = -y_train[flip_ids]\n",
    "\n",
    "    # run linear regression\n",
    "    LR = LinearRegression(fit_intercept=False, )\n",
    "    LR.fit(X_train, y_train)\n",
    "\n",
    "    # calculate in-sample error\n",
    "    y_pred = np.sign(LR.predict(X_train))\n",
    "    e_in = np.sum(y_train != y_pred) / len(y_train)\n",
    "    e.append(e_in)\n",
    "    \n",
    "e = sum(e) / len(e)\n",
    "print('Average in-sample error: ' + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$14.$ Now, transform the training data into the following nonlinear feature vector:\n",
    "\n",
    "$(1, x_1, x_2, x_1x_2, x_1^2, x_2^2)$\n",
    "\n",
    "Find the vector $\\tilde{\\mathbf{w}}$ that corresponds to the solution of Linear Regression, and take it for classification. \n",
    "\n",
    "Which of the following hypotheses is closest to the one you find using Linear Regression on the transformed input? \n",
    "\n",
    "Closest here means agrees the most with your hypothesis (has the most probability of agreeing on a randomly selected point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [ -9.92140857e-01   9.62066238e-04   1.85701329e-03  -1.28203618e-03\n",
      "   1.56198573e+00   1.55271854e+00]\n"
     ]
    }
   ],
   "source": [
    "coefs = []\n",
    "for _ in range(1000):\n",
    "    # Generate the training set\n",
    "    X_train = np.random.uniform(-1, 1, size=(1000, 2))\n",
    "    y_train = np.sign(list(map(f, X_train)))\n",
    "    X_train = np.insert(X_train, 0, 1, axis=1)\n",
    "    X_train = np.insert(X_train, X_train.shape[1], X_train[:, 1] * X_train[:, 2], axis=1)\n",
    "    X_train = np.insert(X_train, X_train.shape[1], X_train[:, 1]**2, axis=1)\n",
    "    X_train = np.insert(X_train, X_train.shape[1], X_train[:, 2]**2, axis=1)\n",
    "    \n",
    "    # Flip 10% of the signs\n",
    "    flip_ids = np.random.choice(1000, 100, replace=False)\n",
    "    y_train[flip_ids] = -y_train[flip_ids]\n",
    "\n",
    "    # run linear regression\n",
    "    LR = LinearRegression(fit_intercept=False)\n",
    "    LR.fit(X_train, y_train)\n",
    "\n",
    "    # bookkeep the weights\n",
    "    coefs.append(LR.coef_)\n",
    "    \n",
    "coef = np.average(coefs, axis=0)\n",
    "print(\"Weights: \" + str(coef))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$15.$ Following Question 14, what is the closest value to the classification out-of-sample error $E_{\\rm out}$ of your hypothesis? \n",
    "\n",
    "Estimate it by generating a new set of 1000 points and adding noise as before. Average over 1000 runs to reduce the variation in your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average out-of-sample error: 0.125684\n"
     ]
    }
   ],
   "source": [
    "e = []\n",
    "for _ in range(1000):\n",
    "    # Generate the training set\n",
    "    X_train = np.random.uniform(-1, 1, size=(1000, 2))\n",
    "    y_train = np.sign(list(map(f, X_train)))\n",
    "    X_train = np.insert(X_train, 0, 1, axis=1)\n",
    "    X_train = np.insert(X_train, X_train.shape[1], X_train[:, 1] * X_train[:, 2], axis=1)\n",
    "    X_train = np.insert(X_train, X_train.shape[1], X_train[:, 1]**2, axis=1)\n",
    "    X_train = np.insert(X_train, X_train.shape[1], X_train[:, 2]**2, axis=1)\n",
    "\n",
    "    # Flip 10% of the signs\n",
    "    flip_ids = np.random.choice(1000, 100, replace=False)\n",
    "    y_train[flip_ids] = -y_train[flip_ids]\n",
    "\n",
    "    # run linear regression\n",
    "    LR = LinearRegression(fit_intercept=False)\n",
    "    LR.fit(X_train, y_train)\n",
    "\n",
    "    # Generate the test set\n",
    "    X_test = np.random.uniform(-1, 1, size=(1000, 2))\n",
    "    y_test = np.sign(list(map(f, X_test)))\n",
    "    X_test = np.insert(X_test, 0, 1, axis=1)\n",
    "    X_test = np.insert(X_test, X_test.shape[1], X_test[:, 1] * X_test[:, 2], axis=1)\n",
    "    X_test = np.insert(X_test, X_test.shape[1], X_test[:, 1]**2, axis=1)\n",
    "    X_test = np.insert(X_test, X_test.shape[1], X_test[:, 2]**2, axis=1)\n",
    "\n",
    "    # Flip 10% of the signs\n",
    "    flip_ids = np.random.choice(1000, 100, replace=False)\n",
    "    y_test[flip_ids] = -y_test[flip_ids]\n",
    "\n",
    "    # run prediction and calculate error\n",
    "    y_pred = np.sign(LR.predict(X_test))\n",
    "    e_out = np.sum(y_pred != y_test) / len(y_pred)\n",
    "    e.append(e_out)\n",
    "    \n",
    "e = sum(e) / len(e)\n",
    "print(\"Average out-of-sample error: \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Questions 18-20, you will play with logistic regression. Please use the following set for training:\n",
    "\n",
    "https://www.csie.ntu.edu.tw/~htlin/mooc/datasets/mlfound_algo/hw3_train.dat\n",
    "\n",
    "and the following set for testing:\n",
    "\n",
    "https://www.csie.ntu.edu.tw/~htlin/mooc/datasets/mlfound_algo/hw3_test.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.genfromtxt('hw3_train.dat')\n",
    "X_train = data[:, :-1]\n",
    "y_train = data[:, -1]\n",
    "\n",
    "data = np.genfromtxt('hw3_test.dat')\n",
    "X_test = data[:, :-1]\n",
    "y_test = data[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$18.$ Implement the fixed learning rate gradient descent algorithm for logistic regression. Run the algorithm with $\\eta = 0.001$ and $T = 2000$. \n",
    "\n",
    "What is $E_{out}(g)$ from your algorithm, evaluated using the 0/1 error on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonzhang/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/Users/jasonzhang/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-sample error: 0.475\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression\n",
    "LR = SGDClassifier(loss='log', fit_intercept=False, learning_rate='constant', eta0=0.001, penalty='none')\n",
    "for _ in range(2000):\n",
    "    idx = np.random.randint(0, X_train.shape[0])\n",
    "    LR.partial_fit(X_train[idx].reshape(1, -1), y_train[idx].reshape(1, -1), classes=np.unique(y_train))\n",
    "y_pred = LR.predict(X_test)\n",
    "e = np.sum(y_pred != y_test) / len(y_pred)\n",
    "print('Out-of-sample error: ' + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$19.$ Implement the fixed learning rate gradient descent algorithm for logistic regression. Run the algorithm with $\\eta = 0.01$ and $T = 2000$.\n",
    "\n",
    "What is $E_{out}(g)$ from your algorithm, evaluated using the 0/1 error on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonzhang/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/Users/jasonzhang/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-sample error: 0.27\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression\n",
    "LR = SGDClassifier(loss='log', fit_intercept=False, learning_rate='constant', eta0=0.01, penalty='none')\n",
    "for idx in range(2000):\n",
    "    idx = np.random.randint(0, X_train.shape[0])\n",
    "    LR.partial_fit(X_train[idx].reshape(1, -1), y_train[idx].reshape(1, -1), classes=np.unique(y_train))\n",
    "y_pred = LR.predict(X_test)\n",
    "e = np.sum(y_pred != y_test) / len(y_pred)\n",
    "print('Out-of-sample error: ' + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$20.$ Implement the fixed learning rate stochastic gradient descent algorithm for logistic regression. Instead of randomly choosing nn in each iteration, please simply pick the example with the cyclic order $n = 1, 2, \\ldots, N, 1, 2, \\ldots$\n",
    "\n",
    "Run the algorithm with $\\eta = 0.001$ and $T = 2000$. What is $E_{out}(g)$ from your algorithm, evaluated using the 0/1 error on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonzhang/anaconda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/Users/jasonzhang/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-sample error: 0.471666666667\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression\n",
    "LR = SGDClassifier(loss='log', fit_intercept=False, learning_rate='constant', eta0=0.001, penalty='none')\n",
    "for i in range(2000):\n",
    "    idx = i % X_train.shape[0]\n",
    "    LR.partial_fit(X_train[idx].reshape(1, -1), y_train[idx].reshape(1, -1), classes=np.unique(y_train))\n",
    "y_pred = LR.predict(X_test)\n",
    "e = np.sum(y_pred != y_test) / len(y_pred)\n",
    "print('Out-of-sample error: ' + str(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
